{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating the data model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading necessary libraries, some must first be installed using pip\n",
    "\n",
    "# System and File Management\n",
    "import sys\n",
    "import os\n",
    "\n",
    "# Data Handling \n",
    "import pandas as pd\n",
    "\n",
    "# Geospatial Analysis\n",
    "import geopandas as gpd\n",
    "from shapely.geometry import Point\n",
    "import haversine \n",
    "\n",
    "# Network Analysis\n",
    "import networkx as nx\n",
    "# Data Loading and Connection\n",
    "import json "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will now look into describing in more detail the process of transforming the collected data into a data model representing network snapshots of Berlin's public transportation system.\n",
    "\n",
    "### Data Cleaning and Reconciliation \n",
    "\n",
    "As mentioned previously, the information gathered from the Fahrplanbücher was transcribed by hand into a series of spreadsheets for each respective year. The raw spreadsheets were then processed using Python\\'s Pandas library, which allows for organising and manipulating the data in table-like structures called dataframes. This had to be done in separate steps. After extracting station name data, OpenRefine was used to reconcile station names with existing Wikidata objects. This reconciliation process aimed to standardise identification and enable potential future connections to the semantic web. We are specifically reconciling with Wikidata objects of the types Berlin S-Bahn station (Q110977521), Berlin U-Bahn station (Q110977120) and tram stop (Q2175765). We then follow the geolocalisation procedure mentioned earlier in section 2.1.1.\n",
    "\n",
    "Once all stations mentioned in the data for a year had been localised we proceed with the step of dividing the data into three linked tables. The three tables are (1) the list of stations, and station attributes (coordinates, type of station, Wikidata-identifiers, what lines these stations are in) (2) the list of lines, and the their attributes (year, type, length in minutes and km where possible, start and end stations) and (3) the list of stop-order, this table references the other two tables with their respective unique ids and includes a column called \"stop-order\" which is a numeric value starting at 1 for each new line and adding 1 for each station in that line. When a new line id is referenced the \"stop-order\" reverts back to 1 before increasing again. These tables are completed for each snapshot. Once all the data for all the snapshots has been completed we merge the tables together, the results can be found in the final-tables folder.\n",
    "\n",
    "### Database Integration\n",
    "\n",
    "To manage and analyse our data efficiently, we are integrating it into a custom-designed database. This database allows for powerful analysis and makes the data easier to share with other researchers in the future. The csv files could have been used to directly create our modelled network; however, it was decided that the project would benefit from uploading the csv files as three separate tables into a SQL database. The main benefit is that the relational data structure of the three linked tables is ideal for a relational database, in this case MySQL. We will be able to write queries that traverse the connections easily, such as \\\"Find stations that were added between 1960 and 1965 and belonged to bus lines\\\". This would be cumbersome to do with multiple CSV files. The query efficiency is also improved in SQL databases as they are optimised to handle large datasets and complex queries. In addition, it is intended to make the data publicly available in the future to allow for reuse, collaboration and improvement. With standardised access mechanisms, a SQL database will be easier to work with in this regard than CSV files.\n",
    "\n",
    "In this geographically referenced database approach, the project also brings in a GIS component. By modelling the geographic data into a common frame of reference, namely decimal coordinates, we can hope to add data to the field of Historical GIS. We are here following the call of Jordi Marti Henneberg to produce locational-temporal databases as a justifiable, self-contained project with its own merit.[^47] The goal is to allow multiple disciplines and interests to be pursued using the data we have prepared in this project.\n",
    "\n",
    "Currently, we operate a local MySQL instance. Finalized CSV files are imported as individual tables within the database. We establish key referencing based on shared node and line IDs between tables. This structure enables us to query the database efficiently, extracting the necessary data for network generation. The process involves constructing a comprehensive dataframe containing all relevant information.\n",
    "\n",
    "[^47]: Jordi Marti-Henneberg, Geographical Information Systems and the Study of History, in: The Journal of Interdisciplinary History 42, 2011, pg. 11."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# code to open file with queried data\n",
    "# for use when no access to database possible\n",
    "\n",
    "df = pd.read_csv('queried_data.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Enrichment\n",
    "\n",
    "The georeferenced nature of our network allows for integration of a variety of external geographic datasets. For example, we have already integrated Berlin district boundaries to assign each node its respective district. Additionally, we could incorporate historical demographic data, land-use maps, or infrastructure information. These integrations will enable us to analyse the transportation network in relation to urban development patterns, socioeconomic factors, and other relevant variables. In 3.2, we will explore how this enriched dataset along with demographic data can be used to address specific research questions about the evolution of Berlin\\'s transport system."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\noahb\\AppData\\Local\\Temp\\ipykernel_7980\\408996106.py:32: UserWarning: CRS mismatch between the CRS of left geometries and the CRS of right geometries.\n",
      "Use `to_crs()` to reproject one of the input geometries to match the CRS of the other.\n",
      "\n",
      "Left CRS: None\n",
      "Right CRS: EPSG:4326\n",
      "\n",
      "  result_gdf = gpd.sjoin(stations_gdf, districts_gdf, how=\"left\", predicate='within')\n"
     ]
    }
   ],
   "source": [
    "# extract_coords function\n",
    "def extract_coords(coord_str):\n",
    "    lon, lat  = map(float, coord_str.split(',')) \n",
    "    return Point(lat, lon)  # Use Shapely's Point \n",
    "\n",
    "# Load station data and apply the `extract_coords` function to create geometries\n",
    "stations_gdf = gpd.GeoDataFrame(df, geometry=df['coordinate_location'].apply(extract_coords))\n",
    "\n",
    "# Load district GeoJSON data\n",
    "districts_gdf = gpd.read_file(\"data-external/lor_ortsteile.geojson\")\n",
    "\n",
    "# Read the GeoJSON file and extract ortsteil values\n",
    "with open('data-external/lor_ortsteile.geojson') as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "ortsteil_values = []\n",
    "for feature in data['features']:\n",
    "    ortsteil_values.append(feature['properties']['OTEIL'])\n",
    "\n",
    "# Get unique values\n",
    "unique_oteil_values = list(set(ortsteil_values))\n",
    "\n",
    "# Create a dictionary mapping bezirk (district) to a list of its ortsteil (subdistricts)\n",
    "bezirk_to_ortsteil = {}\n",
    "for feature in data['features']:\n",
    "    oteil = feature['properties']['OTEIL']\n",
    "    bezirk = feature['properties']['BEZIRK']\n",
    "    bezirk_to_ortsteil.setdefault(bezirk, []).append(oteil) \n",
    "\n",
    "# Perform a spatial join between stations and districts \n",
    "# Keeps station points that fall within districts \n",
    "result_gdf = gpd.sjoin(stations_gdf, districts_gdf, how=\"left\", predicate='within') \n",
    "\n",
    "result_gdf = result_gdf.drop([\"index_right\"], axis=1)\n",
    "\n",
    "# dropping unnecessary columns\n",
    "df = result_gdf.drop([\"gml_id\", \"spatial_name\", \"spatial_alias\", \"spatial_type\", \"FLAECHE_HA\"], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print all unique values in the column 'OTEIL'\n",
    "ortsteile = []\n",
    "for x in df['OTEIL'].unique():\n",
    "    ortsteile.append(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load districts in West Berlin\n",
    "with open(\"data-external/West-Berlin-Ortsteile.json\", \"r\") as infile:\n",
    "    West_Berlin_Ortsteile = json.load(infile)\n",
    "    West_Berlin_Ortsteile = West_Berlin_Ortsteile[\"West_Berlin\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Mitte', 'Friedrichshain', 'Lichtenberg', 'Rummelsburg', 'Rosenthal', 'Niederschönhausen', 'Pankow', 'Prenzlauer Berg', 'Schöneberg', 'Neukölln', 'Alt-Treptow', 'Weißensee', 'Alt-Hohenschönhausen', 'Fennpfuhl', 'Friedrichsfelde', 'Karlshorst', 'Biesdorf', 'Kaulsdorf', 'Mahlsdorf', nan, 'Blankenfelde', 'Müggelheim', 'Köpenick', 'Konradshöhe', 'Oberschöneweide', 'Heinersdorf', 'Grünau', 'Schmöckwitz', 'Plänterwald', 'Baumschulenweg', 'Friedrichshagen', 'Niederschöneweide', 'Johannisthal', 'Adlershof', 'Buch', 'Karow', 'Blankenburg', 'Marzahn', 'Französisch Buchholz', 'Altglienicke', 'Lübars', 'Neu-Hohenschönhausen', 'Falkenberg', 'Wartenberg', 'Wilhelmsruh', 'Märkisches Viertel', 'Malchow', 'Stadtrandsiedlung Malchow', 'Hellersdorf']\n"
     ]
    }
   ],
   "source": [
    "ortsteile_ost = [ort for ort in ortsteile if ort not in West_Berlin_Ortsteile] \n",
    "# print leftover Ortsteile to validate\n",
    "print(ortsteile_ost)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a new column 'east-west' and assign values based on OTEIL\n",
    "df['east-west'] = df['OTEIL'].apply(lambda x: 'west' if x in West_Berlin_Ortsteile else 'east')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Network Model\n",
    "\n",
    "We will employ an L-space graph model to represent Berlin\\'s public transportation network. This model is particularly well-suited because it accurately captures real-world service patterns: stations are connected by an edge if they are consecutive stops on the same line.[^48] There are other models that are used for public transportation systems; however, these add layers of abstraction that are not inherently beneficial for our investigation.\n",
    "\n",
    "Our model will also be a multi-layer L\\' space graph, as we will incorporate different transportation types as distinct node and edge categories. One can think of each transportation system that is modelled as another layer in our network. This type of model for public transportation systems is much less common than L space graphs, which differ due to the presence of only one link type.[^49] This multi-layer approach provides a holistic view of the entire network.\n",
    "\n",
    "When constructing our L' space graph we will be adding our node and edge attributes as well from the data available in the dataframe. These attributes are specifically the node type, labels, and years, as well as the edge types, labels, years and capacity. We will also be adding edge distance, calculated in a straight line in kilometres, although this metric is inherently flawed, particularly for tram and bus routes where the restriction to the road network means that the actual distance could be considerably longer.\n",
    "\n",
    "[^48]: Nam Huynh, Johan Barthelmy, A comparative study of topological analysis and temporal network analysis of a public transport system, in: International Journal of Transportation Science and Technology, 2022, pg.393\n",
    "\n",
    "[^49]: C. von Ferber et al., Network harness: Metropolis public transport, in: Physica A: Statistical Mechanics and Its Applications, 380 2007, pg. 586"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Construction of initial L' space graph\n",
    "\n",
    "# distinction between Klein- and Großprofil lines for u-bahn netzwerk\n",
    "Kleinprofil = {'1', '2', '3', '4', 'A', 'A I', 'A II', 'A III', 'A1', 'A2', 'B', 'B I', 'B II', 'B III', 'B1', 'B2', }\n",
    "Großprofil = {'5', '6', '7', '8', '9','C', 'C I', 'C II', 'D', 'E', 'G'}\n",
    "\n",
    "def create_network_graph(df):\n",
    "    # G used to save graph\n",
    "    G = nx.MultiGraph()\n",
    "\n",
    "    # Add nodes for each stop_id with coordinates as attributes\n",
    "    for index, row in df.iterrows():\n",
    "        stop_id = row[\"stop_id\"]\n",
    "        coordinates = row[\"coordinate_location\"].split(\",\")\n",
    "        year = row[\"year\"]\n",
    "        type = row[\"type\"]\n",
    "        east_west = row[\"east-west\"]\n",
    "        neighbourhood = row[\"OTEIL\"]\n",
    "        district = row[\"BEZIRK\"]\n",
    "\n",
    "\n",
    "        if len(coordinates) == 2:\n",
    "            coordinate_y = float(coordinates[0])\n",
    "            coordinate_x = float(coordinates[1])\n",
    "\n",
    "        # adding nodes with attributes\n",
    "        G.add_node(stop_id, x=coordinate_x, y=coordinate_y, year=year, type=type, east_west=east_west, neighbourhood=neighbourhood, district=district)\n",
    "\n",
    "    # Add edges\n",
    "    for line in set(df[\"line_id\"]):\n",
    "        df_line = df[df[\"line_id\"] == line].sort_values(\"stop_order\")\n",
    "        line_name = df_line.iloc[0][\"line_name\"]  # get line name associated with line_id\n",
    "        \n",
    "        for i in range(len(df_line) - 1):\n",
    "            source = df_line.iloc[i][\"stop_id\"]\n",
    "            target = df_line.iloc[i + 1][\"stop_id\"]\n",
    "            edge_type = df_line.iloc[i][\"type\"]\n",
    "            line_id = df_line.iloc[i][\"line_id\"]\n",
    "            year = df_line.iloc[i][\"year\"]\n",
    "            frequency = df_line.iloc[i][\"frequency\"]\n",
    "            east_west = df_line.iloc[i][\"east_west\"]\n",
    "            \n",
    "            # Create a dictionary with edge attributes, including \"type\"\n",
    "            edge_attributes = {\n",
    "                \"key\": line_id,\n",
    "                \"label\": line_name,\n",
    "                \"type\": edge_type,\n",
    "                \"year\": year,\n",
    "                \"frequency\": frequency,\n",
    "                \"east_west\": east_west\n",
    "            }\n",
    "\n",
    "            # Determine capacity based on \"type\" and \"line_name\"\n",
    "            if edge_type == \"u-bahn\" and line_name in Kleinprofil: # different for Großprofil and Kleinprofil lines\n",
    "                edge_attributes[\"capacity\"] = 750\n",
    "            elif edge_type == \"u-bahn\" and line_name in Großprofil: # different for Großprofil and Kleinprofil lines\n",
    "                edge_attributes[\"capacity\"] = 1000\n",
    "            elif edge_type == \"s-bahn\":\n",
    "                edge_attributes[\"capacity\"] = 1100\n",
    "            elif edge_type == \"strassenbahn\":\n",
    "                edge_attributes[\"capacity\"] = 195\n",
    "            elif edge_type == \"bus\" or edge_type == \"bus (Umlandlinie)\":\n",
    "                edge_attributes[\"capacity\"] = 100\n",
    "            elif edge_type == \"FÃ¤hre\":\n",
    "                edge_attributes[\"capacity\"] = 100\n",
    "            \n",
    "            G.add_edge(source, target, **edge_attributes)\n",
    "\n",
    "    return G\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to add distances between nodes connected by edge \n",
    "# useful for calculation of total line distance and sanity check\n",
    "def add_distance_attribute(graph):\n",
    "    for u, v, data in graph.edges(data=True):\n",
    "        u_coord = (graph.nodes[u][\"y\"], graph.nodes[u][\"x\"])\n",
    "        v_coord = (graph.nodes[v][\"y\"], graph.nodes[v][\"x\"])\n",
    "        distance = haversine.haversine(u_coord, v_coord, unit=\"km\")\n",
    "        data[\"distance\"] = distance\n",
    "        data[\"edge_type\"] = data[\"type\"]\n",
    "\n",
    "\n",
    "    return graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Update the frequency where type is \"s-bahn\" and frequency is 0\n",
    "# handling missing information regarding s-bahn lines\n",
    "df.loc[(df['type'] == 's-bahn') & (df['frequency'] == 0), 'frequency'] = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create network graph\n",
    "G = create_network_graph(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_graph_attributes(G, df):\n",
    "    #set node labels to stop names\n",
    "    stop_names = {row[\"stop_id\"]: row[\"stop_name\"] for index, row in df.iterrows()}\n",
    "    nx.set_node_attributes(G, stop_names, \"node_label\")\n",
    "\n",
    "    return G\n",
    "\n",
    "# add node labels\n",
    "G = add_graph_attributes(G, df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "G = add_distance_attribute(G)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our network snapshots have now been created. However, there\\'s some uncertainty about whether stations appearing in multiple snapshots truly represent the same physical location. Minor variations in station names, localisation inaccuracies, or slight real-world shifts in station position over time can create this issue.\n",
    "\n",
    "To ensure that we can accurately analyse how a station\\'s role in the network has changed, we can merge nodes that represent the same station across different years into a single node. This approach is in line with TVG modelling of networks. It is common practice to have an \"underlying graph\", which flattens the time dimension and indicates all nodes that have relations at some time in T.[^50] We will first approach this by shortening the decimal coordinates of the nodes to three decimal places and then grouping across snapshots using just the locational data and type of node. Merging nodes across snapshots also allows us to take a look into the persistence of nodes, this will provide further insights into the stability of the system we are investigating. We are not merging edges across time periods because the frequency values will be specific to a snapshot.\n",
    "\n",
    "[^50]: A. Casteigts et al, pg. 350."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine nodes based on x, y, type and save to dictionary, add year and node labels as attributes\n",
    "combined_nodes = {}\n",
    "for node in G.nodes(data=True):\n",
    "    key = (round(node[1]['x'], 3), round(node[1]['y'], 3), node[1]['type']) \n",
    "    if key not in combined_nodes:\n",
    "        combined_nodes[key] = {'node_labels': set(), 'year': set(), \"east_west\": set(), \"postcode\": set(), \"neighbourhood\": set(), \"district\": set()}\n",
    "    combined_nodes[key]['node_labels'].add(node[1]['node_label'])\n",
    "    combined_nodes[key]['year'].add(node[1]['year'])\n",
    "    combined_nodes[key]['east_west'].add(node[1]['east_west'])\n",
    "    combined_nodes[key]['neighbourhood'].add(node[1]['neighbourhood'])\n",
    "    combined_nodes[key]['district'].add(node[1]['district'])\n",
    "\n",
    "# Create a new graph with combined nodes\n",
    "H = nx.MultiGraph()\n",
    "# using MultiGraph to have parrallel edges for each year & line because they all have their own frequencies\n",
    "for key, data in combined_nodes.items():\n",
    "    H.add_node(key, x=key[0], y=key[1], type=key[2], node_labels=list(data['node_labels']), years=list(data['year']), east_west=data[\"east_west\"], neighbourhood=data[\"neighbourhood\"], district=data[\"district\"])\n",
    "\n",
    "# Add edges to the new graph \n",
    "for edge in G.edges(data=True):\n",
    "    node1, node2, edge_attrs = edge\n",
    "    key1 = (round(G.nodes[node1]['x'], 3), round(G.nodes[node1]['y'], 3), G.nodes[node1]['type'])\n",
    "    key2 = (round(G.nodes[node2]['x'], 3), round(G.nodes[node2]['y'], 3), G.nodes[node2]['type'])\n",
    "\n",
    "    if key1 in combined_nodes and key2 in combined_nodes:\n",
    "        H.add_edge(key1, key2, key=str(edge_attrs), weight=1, **edge_attrs) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "H = add_distance_attribute(H)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "for u, v, d in H.edges(data=True):\n",
    "    if H.nodes[u]['east_west'] == H.nodes[v]['east_west']:\n",
    "        value = H.nodes[u]['east_west']  # 'east' or 'west'\n",
    "        d[\"traverses\"] = str(value)\n",
    "    else:\n",
    "        d[\"traverses\"] = str(d[\"east_west\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Addressing Data Uncertainties\n",
    "\n",
    "In theory, we should be left with the same number of edges but fewer nodes. We see however, that there are now fewer edges in our network. The most probable reason for this is that, if multiple nodes in G share these rounded attributes, they\\'ll be merged into a single node in H. Consequently, edges that existed between these nodes in G will disappear in H, as they would become self-loops. With the following code we see that the number of self-loops is double of the number of edges we lost, establishing this was the cause of the loss of edges."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19514\n",
      "26867\n",
      "4031\n",
      "26763\n"
     ]
    }
   ],
   "source": [
    "print(G.number_of_nodes())\n",
    "print(G.number_of_edges())\n",
    "print(H.number_of_nodes())\n",
    "print(H.number_of_edges())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "313\n"
     ]
    }
   ],
   "source": [
    "# check for self-loops that are created by removing specificity in coordinates\n",
    "self_loops_G = list(nx.selfloop_edges(G))\n",
    "self_loops_H = list(nx.selfloop_edges(H))\n",
    "print(len(self_loops_G))\n",
    "print(len(self_loops_H))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The impact of this is negligible when considering the total amount of edges in our network. The fault originally lies in the localisation of the stations being incorrect. While few edges were lost, ensuring accurate analysis requires us to track down these self-loops, as they indicate potential errors that could skew results about a station\\'s connectivity over time."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Weight Calculation\n",
    "\n",
    "We now turn to the issue of assigning a unitary weight to our edges. In transportation networks, edge weights help us understand the relative importance of routes. Higher weights might indicate higher passenger traffic or greater strategic value to the network. Other public transportation networks have assigned edge weight based on the capacity of the vehicle, the distance between nodes or the number of overlapping services.[^51] Our project has two values that can be used to measure weight, these being capacity and frequency. There is no set way in the literature on public transportation system network modelling to calculate an edge weight, so we will test two different figures. The first is the value of passengers-per-hour. This value provides a figure for the number of passengers that could be transported in an hour. To calculate this, we calculate the number of services per hour given the frequency and then multiply this number by the capacity of the transportation type. The assumption here is that the frequency is set for an hour. Based on the available data, this is however not the case, as frequency changes occur at random times. This is therefore a very inaccurate description, given that the frequency might have changed a minute before or after our 7:30 mark.\n",
    "\n",
    "Because of the imprecision involved in calculating raw passenger-per-hour figures, we will use normalisation to aid our analysis. Normalising our capacity and frequency values puts them on a consistent scale (0 to 1). This lets us easily compare the relative importance of different edges within our network. For example, if one route has a normalised capacity of 0.8 and another has 0.2, we understand the first route has four times the capacity of the second. Normalisation will help us identify potential bottlenecks and key routes within the network, regardless of potentially imprecise absolute capacity figures. In order to maintain flexibility, we will keep the original capacity and frequency values and assign two further attributes to our edges: normalised capacity and normalised frequency. From these two values we will calculate a weight which will be more relative in nature. We do this by adding the two normalised values, as neither of the two values should be given increased importance over the other. A main benefit for normalisation of edge weights for our analysis is that because we are normalising based on the min and max values throughout all snapshots together, we will be able to compare our weights across snapshots. For example, we will be able to track changes in average edge weights for our snapshots to see if the average edge weight changed during the period under observation.\n",
    "\n",
    "[^51]: Tanuja Shanmukhappa, Ivan Wang-Hei Ho, K.Tse Chi, Recent development in public transport network analysis from the complex network perspective, in: IEEE Circuits and Systems Magazine 19, no. 4, 2019, pg. 55."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating weight for edges based on frequency and capacity\n",
    "# passengers-per-hour = service frequency per hour * capacity of a vehicle\n",
    "for u, v, key in H.edges(keys=True):  # Using keys will distinguish parallel edges\n",
    "    frequency = int(H[u][v][key]['frequency'])\n",
    "    capacity = int(H[u][v][key]['capacity'])\n",
    "\n",
    "    if frequency != 0:\n",
    "        services_per_hour = 60 / frequency\n",
    "    else:\n",
    "        services_per_hour = 0\n",
    "    weight = services_per_hour * capacity\n",
    "    H[u][v][key]['passengers-per-hour'] = weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculating improved weight score using normalised frequency and capacity measures\n",
    "# weight = norm_capacity + norm_frequency\n",
    "def normalize(values):\n",
    "    max_value = max(values)\n",
    "    min_value = 0\n",
    "    return [(value - min_value) / (max_value - min_value) for value in values]\n",
    "\n",
    "capacities = [edge[2]['capacity'] for edge in H.edges(data=True)]\n",
    "frequencies = [int(edge[2]['frequency']) for edge in H.edges(data=True)]\n",
    "\n",
    "normalized_capacities = normalize(capacities)\n",
    "normalized_frequencies = normalize(frequencies)  \n",
    "\n",
    "for i, edge in enumerate(H.edges(data=True)):\n",
    "    edge[2]['normalised_capacity'] = normalized_capacities[i]\n",
    "    edge[2]['normalised_frequency'] = normalized_frequencies[i]\n",
    "\n",
    "for x, y, d in H.edges(data=True):\n",
    "    weight = d['normalised_capacity'] + d['normalised_frequency']\n",
    "    d['weight'] = weight"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data model criticism\n",
    "\n",
    "Although we have chosen a model that reflects the real-world structure and behaviour of the public transportation system, we are still dealing with an abstraction in which elements of the real-world transportation system are lost. It is important that we highlight these and are certain of and clear about their consequences.\n",
    "\n",
    "### Distance in time\n",
    "\n",
    "The distance of one station to another in terms of travel time of service is a common data attribute of edges that are often modelled for public transportation data. The information is available in the Fahrplanbücher but it was not included in our model. The main reason for this was that it was not considered feasible given that our network has over 25,000 edges. The importance of this figure was considered not vital to the exploration of how the public transportation system evolved over such a large time scale. Previous work which includes this information does not investigate evolving public transportation networks, but rather the temporality of a transport network at one state, and for this type of analysis the figure would have been vital.\n",
    "\n",
    "The more distanced perspective in our project did lend itself better to capturing the overall travel time of the line, rather than the travel time between stops. Capturing this information from the sources was much less time intensive. However, the network model we have selected does not have a suitable place to store this information as the lines do not exist as entities themselves, but only as attributes of the edges. A different network model based on this captured data would be able to potentially analyse the total travel times of all lines in different snapshots and thereby gain another perspective into how the system changed.\n",
    "\n",
    "### Directionality\n",
    "\n",
    "Directionality presents a limitation in our undirected network model. The model does not inherently capture the direction of travel on lines, limiting analysis of directional passenger flows and how those might have shifted over time due to evolving patterns of urban development. The complexity of adding directionality to our network would have been negligible to the benefits it would have brought to the analysis of the potential of dynamic NA to historical research at this stage. Still, we need to be aware that we have an incomplete picture of the network as currently modelled. There are two important aspects that this undirected model does not capture. Firstly, there are two frequency values for the service at 7:30, these are for most of the times the same but are sometimes different depending on the direction the service sets out from. Secondly, in our 1989 Fahrplanbuch for West Berlin, bus stations differed depending on direction. Meaning that some stops were only serviced for a specific direction. In our model we have only captured the stations in one direction. To capture this and allow for future incorporation of the missing information, we have applied a standard rule for how the data was captured. We always capture the information from the first table (which shows one line direction) in our Fahrplanbuch and in our database we have for each line a start and end station, which allows us to infer directionality, even though at this stage we are treating it as undirected. This allows for the possibility to differentiate between lines based on direction, and thereby including the second frequency value and the stations serviced in the other direction.\n",
    "\n",
    "## Conclusion on datafication/abstraction\n",
    "\n",
    "The process of transforming historical records of Berlin\\'s public transportation system into a network model involves careful datafication and necessary abstractions. With this network model we attempt to limit the abstraction so as not to remove the object of analysis unnecessarily far out of its context, which makes it harder for our analysis to inform our understanding of the system. Still, we need to critically reflect on what was gained and lost in this process to understand the analytical possibilities and limitations.\n",
    "\n",
    "By transforming the available information into a L\\'-space graph model we keep an understandable structure that reflects the real-world structure of the transportation system, with stations as nodes and lines as edges. This makes the network visually and mathematically comprehensible. Capturing different transportation types within one multilayer model allows us to analyse the overall network behaviour, including interactions between trams, buses, U-Bahn, and S-Bahn lines. We provided substantial node and edge attributes (like type, label, year, capacity, frequency and weight), enabling us to track changes over time and identify patterns based on specific characteristics. Additionally, the geographically referenced database we have created and are querying aligns with the goals of historical GIS, providing a foundation for locational-temporal analysis that could be of interest to multiple disciplines.\n",
    "\n",
    "There is substantial information that is not captured in our network model. One such aspect is directionality, the model does not inherently capture the direction of travel on lines, limiting analysis of directional flows and the potential to introduce temporality within network snapshots. Another limitation is our use of a single frequency snapshot (7:30 AM), which risks misrepresenting the true importance of lines with varying service levels throughout the day. We know from the inspection of the sources that frequency changed rapidly throughout the day for some lines. The changes often occurred during four periods; the morning peak frequency, the midday frequency, the afternoon peak frequency and the evening frequency. An extension of the project could easily incorporate further frequency values for other times of the day, bringing an element of temporality into our analysis. The current focus on Berlin\\'s transport network excludes connections to the surrounding regional train system, potentially understating the network\\'s overall reach. This has only a very limited impact on the network in the West, which would not have been connected to the regional network for most of the period under observation. If we were to extend our analysis into the post-1989 era, we may have had to include the regional train system in our network in order to maintain our closed-world assumption. Critically, incomplete records, particularly for tram and bus stops, result in a simplified network that might not account for all transfer points or local routes. Because this issue arises from the information in the sources available to us, the only possibility of completing the records would be if other sources had this information, which so far has not been the case in the associated research.\n",
    "\n",
    "There are multiple analytical possibilities with the network model we have created. Primarily, we will explore network evolution by tracking changes in network structure over time, including the addition/removal of stations and lines, and the development of the multi-modal system. As part of this, we will calculate various centrality measures to identify critical stations or lines based on different criteria (e.g., connectivity, betweenness). We will also be investigating the impact of historical disruptions on network robustness, potentially highlighting vulnerable areas. While limited by imprecision, the normalised capacity and frequency attributes allow us to explore relative capacity differences across the network, paying specific attention to how these change during the period under observation for both East and West Berlin.\n",
    "\n",
    "There are analytical limitations that we will have to contend with for the time being. Specifically, the incomplete capacity data makes it difficult to perform precise analyses focused on absolute passenger capacities. Our single-frequency snapshots also limit our understanding of how service levels fluctuate throughout the day or week, however with our focus on evolution this will not have an impact on our ability to investigate change over longer periods. The network model also does not capture individual passenger journeys, making it unsuitable for analysing route choices or granular travel patterns.\n",
    "\n",
    "In conclusion, the datafication of historical records into a network model offers the potential for valuable insights into the evolution and structure of Berlin\\'s public transportation system. Acknowledging the inherent abstractions and limitations, this model provides a robust foundation for diverse analyses within the constraints of available data. The next section will begin this analysis by describing the changes in the network using simple NA metrics. We also hold the promise that future work could focus on expanding data sources to address limitations and enrich the model\\'s capabilities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saving network:\n",
    "# need to convert list attribute values to json strings to save\n",
    "for node, data in H.nodes(data=True):\n",
    "    for key, value in data.items():\n",
    "        if isinstance(value, set) or isinstance(value, list):  # Include lists\n",
    "            data[key] = \",\".join(str(item) for item in value)  # Convert items to strings\n",
    "\n",
    "for u, v, data in H.edges(data=True):\n",
    "    for key, value in data.items():\n",
    "        if isinstance(value, set) or isinstance(value, list): \n",
    "            data[key] = \",\".join(str(item) for item in value) \n",
    "\n",
    "nx.write_graphml(H, \"base-graph.graphml\")\n",
    "nx.write_gexf(H, \"base-graph.gexf\") "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}